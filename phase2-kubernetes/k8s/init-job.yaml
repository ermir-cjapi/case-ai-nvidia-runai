apiVersion: batch/v1
kind: Job
metadata:
  name: llm-model-download
  namespace: default
spec:
  template:
    metadata:
      labels:
        app: model-download
    spec:
      restartPolicy: OnFailure
      containers:
      - name: download
        image: python:3.10-slim
        command:
          - /bin/bash
          - -c
          - |
            echo "Installing dependencies..."
            pip install --no-cache-dir transformers huggingface-hub accelerate
            
            echo "Downloading model to /model..."
            python3 -c "
            from huggingface_hub import snapshot_download
            import os
            
            model_id = os.getenv('MODEL_ID', 'meta-llama/Llama-3.2-3B-Instruct')
            token = os.getenv('HF_TOKEN', None)
            
            # Debug: Check if token exists
            if token:
                print(f'Token found: {token[:7]}...{token[-4:]} (length: {len(token)})')
            else:
                print('WARNING: No HF_TOKEN environment variable found!')
                print('Available env vars:', [k for k in os.environ.keys() if 'HF' in k or 'TOKEN' in k])
            
            print(f'Downloading {model_id}...')
            snapshot_download(
                repo_id=model_id,
                local_dir='/model',
                local_dir_use_symlinks=False,
                token=token,
                resume_download=True
            )
            print('Download complete!')
            "
            
            echo "Model downloaded successfully!"
            ls -lh /model/
        env:
        - name: MODEL_ID
          value: "meta-llama/Llama-3.2-3B-Instruct"
          # Alternative: "microsoft/Phi-3-mini-4k-instruct" (no token required)
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-token
              key: token
              optional: false  # Fail if token is missing
        volumeMounts:
        - name: model-storage
          mountPath: /model
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: llm-model-storage
  backoffLimit: 3
